{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analytics\n",
    "Twitter data can be useful in a number of different ways for journalism, such as helping to identify events, to understand the aggregate flows and trends of information, or to locate key sources within the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Makes it so that you can scroll horizontally to see all columns of an output DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Make it so urls and tweets won't get truncated when we print them out\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# This magic function allows you to see the charts directly within the notebook. \n",
    "%matplotlib inline\n",
    "\n",
    "# This command will make the plots more attractive by adopting the commone style of ggplot\n",
    "matplotlib.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamergate\n",
    "Andy Baio collected thousands of tweets over a period of 72 hours which used the #Gamergate hashtag. His analysis is [here](https://medium.com/message/72-hours-of-gamergate-e00513f7cf5d#.c12plmtcf) but below we'll start from his raw data and do some of our own analysis. You can download the data [here](https://www.dropbox.com/s/5zeuic9qr8v8y4n/gamergate_tweets_hydrated.csv?dl=0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_df = pd.read_csv(\"Data/gamergate_tweets_hydrated.csv\", parse_dates=\"created_at\")\n",
    "\n",
    "# Print out the column headings to see what kind of data we have\n",
    "gg_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hashtag conversation on Twitter can be characterized in different ways: \n",
    "1. How many tweets were sent and by how many unique users suggests how broad or concentrated the conversation is; \n",
    "2. The number of original tweets vs. retweets can indicate how much new information is being created in comparison to information that is being passed along; \n",
    "3. The number of tweets that are replies directly to another users can be a measure of how \"chatty\" the event is;  \n",
    "4. The distribution of the conversation across different languages could indicate different interest groups as well as if the event has spread internationally;\n",
    "5. Metrics can also be aggregated at the level of users. The total number of retweets, favorites, or followers for each user could indicate \"important\" or at least \"interesting\" people within the conversation\n",
    "\n",
    "Let's walk through each of these analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of tweets sent\n",
    "gg_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that this number differs from the number that Andy indicates in the original blog post he wrote. The difference stems from the fact that the current data we're analyzing was \"re-hydrated\" using a tool called [Twarc](https://github.com/edsu/twarc). Twitter's TOS do not allow you to redistribute tweets. You can only share tweet IDs, and all of the tweet metadata has to be re-hydrated or re-constituted from those tweet IDs. The downside is that some people have deleted their accounts or deleted the original tweets (especially for a controversial topic like this). That means we can't rehydrate all of the original tweets. In fact we only have 215,153 which is about 68%. For this reason it's important to be timely in collecting Twitter data if you want to publish news with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of Unique screen names that sent tweets\n",
    "gg_df[\"user_screen_name\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How many tweets were sent and by how many unique users suggests how broad or concentrated the conversation is.**\n",
    "\n",
    "To understand how broad vs concentrated the conversation is a histogram of # users vs. # tweets  could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vc = gg_df[\"user_screen_name\"].value_counts()\n",
    "# By setting the bin size to the max value count that means each bin will correspond to a single value, the normed=True parameter makes the y-axis a proportion\n",
    "plt.hist(vc, bins=vc.max(), normed=True)\n",
    "\n",
    "# Crop the plot to show show upto 10\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "# Make it bigger\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The number of original tweets vs. retweets can indicate how much new information is being created in comparison to information that is being passed along;**\n",
    "\n",
    "To calculate the number of original tweets versus retweets are in the data we can do some counting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of Original Tweets\n",
    "# if reweet_id is null then it's original, otherwise the tweet is a retweet of the given retweet_id\n",
    "print \"Num Retweets: %d, which is %.2f%% of total.\" % (gg_df[\"reweet_id\"].count(), 100* float(gg_df[\"reweet_id\"].count()) / gg_df.shape[0])\n",
    "print \"Num Original Tweets: %d, which is %.2f%% of total.\" % (gg_df.shape[0] - gg_df[\"reweet_id\"].count(), 100* float(gg_df.shape[0] - gg_df[\"reweet_id\"].count()) / gg_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The number of tweets that are replies directly to another users can be a measure of how \"chatty\" the event is;**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of Reply Tweets\n",
    "# in_reply_to_status_id is not null if the tweet is a reply to another tweet. \n",
    "print \"Num Replies: %d, which is %.2f%% of total.\" % (gg_df[\"in_reply_to_status_id\"].count(), 100* float(gg_df[\"in_reply_to_status_id\"].count()) / gg_df.shape[0])\n",
    "print \"Num Non-Replies: %d, which is %.2f%% of total.\" % (gg_df.shape[0] - gg_df[\"in_reply_to_status_id\"].count(), 100* float(gg_df.shape[0] - gg_df[\"in_reply_to_status_id\"].count()) / gg_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The distribution of the conversation across different languages could indicate different interest groups as well as if the event has spread internationally;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram across languages (\"lang\" is the language of the account not necessarily of the message of the tweet)\n",
    "vcounts = gg_df[\"lang\"].value_counts(ascending=True)\n",
    "print vcounts\n",
    "\n",
    "# Because the \"en\" and \"und\" variables will dominate the bar chart, we remove them before plotting\n",
    "del vcounts[\"en\"]\n",
    "del vcounts[\"und\"]\n",
    "vcounts.plot(kind=\"barh\")\n",
    "\n",
    "# Make it bigger\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Metrics can also be aggregated at the level of users. The total number of retweets, favorites, or followers for each user could indicate \"important\" or at least \"interesting\" people within the conversation**\n",
    "\n",
    "Let's look at activity aggregated by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_user_grouped = gg_df.groupby(\"user_screen_name\")\n",
    "print \"Top 50 Users by # Tweets\"\n",
    "top_50_users = gg_user_grouped.size().sort_values(ascending=False)[0:50]\n",
    "top_50_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just how active were those top 50 users? Let's tabulate the average and median # of RTs for each of the people in the top 50 most active users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First count up the number of tweets from those top users\n",
    "gg_top_50_users_df = gg_df[gg_df[\"user_screen_name\"].isin(top_50_users.index.values)]\n",
    "print \"%d tweets from top 50 users \\n\" % gg_top_50_users_df.shape[0]\n",
    "gg_top_50_grouped = gg_top_50_users_df.groupby(\"user_screen_name\")\n",
    "\n",
    "# Top 50 most active users ranked by average RTs / tweet\n",
    "print \"Top 50 most active users ranked by average RTs / tweet\"\n",
    "print gg_top_50_grouped[\"retweet_count\"].aggregate(np.mean).sort_values(ascending=False)\n",
    "print \"\\n\"\n",
    "\n",
    "# Top 50 users in terms of median RTs\n",
    "print \"Top 50 most active users ranked by median RTs / tweet\"\n",
    "print gg_top_50_grouped[\"retweet_count\"].aggregate(np.median).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: How can we adapt the code above to compute the mean and median number of favorites across all users? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pulse of the Event\n",
    "What's the shape of the event in terms of the hashtags that are used? \n",
    "\n",
    "Let's first examine the set of hashtags that are used at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hastag trend\n",
    "# The hashtags field can have multiple hashtags stuffed into it, separated by a space so we need to parse those out separately to be able to count them\n",
    "hashtags_list = []\n",
    "\n",
    "def parse_hashtags(hashtags):\n",
    "    #print hashtags\n",
    "    hashtags_list.extend(hashtags.split(\" \"))\n",
    "    \n",
    "gg_df[\"hashtags\"].dropna().map(parse_hashtags)\n",
    "\n",
    "hashtags_df = pd.DataFrame(hashtags_list, columns=[\"hashtag\"])\n",
    "print \"Number of unique hashtags: %d \" % hashtags_df[\"hashtag\"].unique().shape[0]\n",
    "print \"\\nTop Ten Hashtags:\"\n",
    "print hashtags_df[\"hashtag\"].value_counts()[0:10]\n",
    "\n",
    "# Convert all the hashtags to lowercase since otherwise we have variations based on capitalization\n",
    "print \"\"\n",
    "hashtags_df[\"hashtag\"] = hashtags_df[\"hashtag\"].map(lambda x: x.lower())\n",
    "print \"Number of unique hashtags: %d \" % hashtags_df[\"hashtag\"].unique().shape[0]\n",
    "print \"\\nTop Ten Hashtags:\"\n",
    "print hashtags_df[\"hashtag\"].value_counts()[0:10]\n",
    "\n",
    "print \"\"\n",
    "top_ten_hashtags = hashtags_df[\"hashtag\"].value_counts()[0:10].index.values\n",
    "print top_ten_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these hashtags over time so we can see the shape of how they were used and if there are any patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need both the hashtags and the creation date in the same list to plot one against the other\n",
    "# Here just tabulate for the top 10 hashtags\n",
    "hashtags_list = []\n",
    "\n",
    "def parse_hashtags(row):\n",
    "    #print hashtags\n",
    "    htags = row[\"hashtags\"].split(\" \")\n",
    "    for h in htags:\n",
    "        if h in top_ten_hashtags:\n",
    "            hashtags_list.append([h.lower(), row[\"created_at\"]])\n",
    "    \n",
    "gg_df.dropna(subset=[\"hashtags\"]).apply(parse_hashtags, axis=1)\n",
    "\n",
    "# Create a data frame from the list\n",
    "hashtags_df = pd.DataFrame(hashtags_list, columns=[\"hashtag\", \"created_at\"])\n",
    "# Need to parse the created_at field as a datetime\n",
    "hashtags_df[\"created_at\"] = pd.DatetimeIndex(hashtags_df[\"created_at\"])\n",
    "# Now generate the histogram\n",
    "hashtags_df.hist(column=\"created_at\", by=\"hashtag\", bins=72, figsize=(12,12), sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Identifying Breaking News Content\n",
    "Besides the people participating in a conversation we may also be interested in identifying key content. This can be very helpful for finding content that's relevent in breaking news situations. The data used below comes from the day of the DC Navy Yard Shooting from 2013 and can be downloaded [here](https://www.dropbox.com/s/m6dlp6oacyt8vhi/navyyard_tweets_hydrated.csv?dl=0). \n",
    "\n",
    "- Most RTed Tweets\n",
    "- Most Favorited Tweets\n",
    "- Most RTed Images\n",
    "- Most Favorited Images\n",
    "- Most Replied-to Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notes on data: \"I collected data using the public API's search endpoint, using the Navy Yard's coordinates as a center point and a mile radius. I also backfilled users found through this query using the user timeline endpoint. This gives me tweets leading up to the event.\" What are the implications of using only geotagged tweets?\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "ny_df = pd.read_csv(\"Data/navyyard_tweets_hydrated.csv\", parse_dates=\"created_at\")\n",
    "\n",
    "# The pytz allows us to convert from Universal time to eastern time (Note: special thanks to Jennifer Stark for this code)\n",
    "local_tz = pytz.timezone('US/Eastern')\n",
    "def utc_to_local(row):\n",
    "    # Parse the string UTC date into a datetime python object\n",
    "    utc_dt = datetime.datetime.strptime(row, \"%Y-%m-%d %H:%M:%S\")\n",
    "    # Change the timezone to eastern and output the datetime as a string again\n",
    "    return utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "ny_df[\"created_at\"] = ny_df[\"created_at\"].apply(utc_to_local)\n",
    "\n",
    "# We know the first tweet relating to the event was at about 8:30am on Sept 16th so let's filter for that.\n",
    "ny_df = ny_df[ny_df[\"created_at\"] > \"2013-09-16 08:30:00\"]\n",
    "\n",
    "ny_df[[\"created_at\", \"text\"]]\n",
    "#ny_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ny_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ny_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good filter to find original breaking news content is to look for tweets that are NOT retweets, but that have been retweeted or favorited themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for non-RTs, and sort by RT count\n",
    "ny_original_df = ny_df[ny_df[\"reweet_id\"].isnull()]\n",
    "ny_original_df.sort_values([\"retweet_count\"], ascending=False)[0:10][[\"tweet_url\", \"text\", \"retweet_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for non-RTs, and sort by favorite count\n",
    "ny_original_df = ny_df[ny_df[\"reweet_id\"].isnull()]\n",
    "ny_original_df.sort_values([\"favorite_count\"], ascending=False)[0:10][[\"tweet_url\", \"text\", \"favorite_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for non-RTs, and for tweets with images, and sort by RT count\n",
    "ny_original_df = ny_df[ny_df[\"reweet_id\"].isnull()]\n",
    "ny_original_df = ny_original_df.dropna(subset=[\"media\"])\n",
    "ny_original_df = ny_original_df.sort_values([\"retweet_count\"], ascending=False)[0:10][[\"media\", \"text\", \"retweet_count\"]]\n",
    "ny_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can review some of these images (sorted by RT counts)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.display import display\n",
    "images = []\n",
    "for i in np.arange(0,10):\n",
    "    images.append(Image(url=ny_original_df.iloc[i].media))\n",
    "    \n",
    "display(*images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter for non-RTs, and for tweets with images, and sort by Favorite count\n",
    "ny_original_df = ny_df[ny_df[\"reweet_id\"].isnull()]\n",
    "ny_original_df = ny_original_df.dropna(subset=[\"media\"])\n",
    "ny_original_df = ny_original_df.sort_values([\"favorite_count\"], ascending=False)[0:10][[\"media\", \"text\", \"favorite_count\"]]\n",
    "ny_original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in np.arange(0,10):\n",
    "    images.append(Image(url=ny_original_df.iloc[i].media))\n",
    "    \n",
    "display(*images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort by most replied to tweet\n",
    "ny_repliedto_df = ny_df[ny_df[\"in_reply_to_status_id\"].notnull()]\n",
    "print ny_repliedto_df[\"in_reply_to_status_id\"].value_counts().sort_values(ascending=False)\n",
    "# Apparently no tweet was replied to more than once in this dataset\n",
    "ny_repliedto_df[[\"in_reply_to_status_id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Analysis\n",
    "An important aspect of social media is that people are linked to other people. Those links can help define groups of people (e.g. if a set of people are all interconnected), or help in identifying central participants who may play key information roles (e.g. if one user is connected to many others, or talks to many others). The strength of a connection between users could indicate how well they know each other. \n",
    "\n",
    "A great library for doing network analysis is called NetworkX, here's the documentation: [https://networkx.github.io/documentation/latest/](https://networkx.github.io/documentation/latest/)\n",
    "\n",
    "We'll cover some basics of using NetworkX here, including:\n",
    "- How to create a graph with nodes and edges, annotate nodes and edges, draw a graph\n",
    "- How to construct a graph from the reply network of users\n",
    "- How to calculate some basic centrality measures for identifying \"interesting\" nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g = nx.Graph()\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.nodes()\n",
    "print g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.add_node(1)\n",
    "print g.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.add_nodes_from([2,3])\n",
    "print g.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.add_edge(1,2)\n",
    "g.add_edges_from([(2,3), (1,3)])\n",
    "print g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"# Nodes: \", g.number_of_nodes()\n",
    "print \"# Edges: \", g.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add metadata to edges or nodes, like a weight value for an edge, or a name for a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g[1][2][\"weight\"] = 5\n",
    "g[1][3][\"weight\"] = 10\n",
    "print g[1]\n",
    "\n",
    "g.node[1][\"name\"] = \"Nick\"\n",
    "print g.node[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can draw the labeled graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a layout based on a spring force algorithm\n",
    "nx.draw(g, node_color=\"#ffaaaa\", with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the Navy Yard event, let's see who is talking to whom in terms of @ replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's construct a graph from the reply network of users\n",
    "# Filter out anyone who never responded to anyone\n",
    "ny_df_filtered = ny_df.dropna(subset=[\"in_reply_to_screen_name\"])\n",
    "print \"# Users:\", ny_df_filtered[\"user_screen_name\"].append(ny_df_filtered[\"in_reply_to_screen_name\"]).unique().shape[0]\n",
    "# To determine unique users we must consider user_screen_name as well as in_reply_to_screen_name fields\n",
    "unique_users = ny_df_filtered[\"user_screen_name\"].append(ny_df_filtered[\"in_reply_to_screen_name\"]).unique()\n",
    "\n",
    "# Create a graph\n",
    "ny_g = nx.Graph()\n",
    "# for each unique user, add a node to the graph\n",
    "for n in unique_users:\n",
    "    ny_g.add_node(n)\n",
    "\n",
    "for i in ny_df_filtered.index:\n",
    "    n1 = ny_df_filtered.loc[i][\"user_screen_name\"]\n",
    "    n2 = ny_df_filtered.loc[i][\"in_reply_to_screen_name\"]\n",
    "    # If it already has the edge, then just increment the weight; otherwise add a new edge with weight = 1\n",
    "    if ny_g.has_edge(n1,n2):\n",
    "        ny_g[n1][n2][\"weight\"] += 1\n",
    "    else:\n",
    "        ny_g.add_edge(n1,n2,weight=1)\n",
    "\n",
    "# Lots of parameters to tweak for the visualization\n",
    "nx.draw(ny_g, node_color=\"#ff8888\", node_size=500, alpha=.8, with_labels=False, font_size=10, pos=nx.spring_layout(ny_g, k=(3/np.sqrt(len(ny_g.nodes()))), weight=\"weight\", iterations=70))\n",
    "plt.gcf().set_size_inches(12,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at the most connected people (in terms of the reply network we've created) by calculating the degree of each node. The degree of a node measures the number of incident edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.degree(ny_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Centrality measures](https://networkx.github.io/documentation/latest/reference/algorithms.centrality.html) can be used to calculate the importance of nodes in various ways. For instance degree centrality for a node is the fraction of nodes it is connected to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The degree_centrality function returns a dictionary, so we use the from_dict pandas function to create a dataframe from that. \n",
    "dc = pd.DataFrame.from_dict(nx.degree_centrality(ny_g), orient=\"index\")\n",
    "dc.columns=[\"deg_centrality\"]\n",
    "dc.sort_values(\"deg_centrality\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What other questions could we ask of this data? \n",
    "If there's time let's brainstorm a few and see if we can work out the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
